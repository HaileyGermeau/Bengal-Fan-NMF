{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal as sig\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "from pathlib import Path  # pathlib is seriously awesome!\n",
    "import sys\n",
    "import sklearn.decomposition as sld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRS_file=pd.read_csv('All_RSC_Data.csv')\n",
    "rs_file=pd.ExcelFile('2023-07-14_random_sample.xlsx')\n",
    "#rs stands for random sample\n",
    "rs_sheet_info=pd.read_excel(rs_file,'samples-354-U1449_U1450_U1451_U')\n",
    "rs_sheet_lith=pd.read_excel(rs_file,'random_sample')\n",
    "#rs_sheet = the specific sheet using to merge\n",
    "MiddleDepth=(rs_sheet_info['Top depth CSF-A (m)']+rs_sheet_info['Bottom depth CSF-A (m)'])/2\n",
    "rs_sheet_info.insert(11,'depth-m',MiddleDepth)\n",
    "#calculating middle depth from top and bottom depth of core section and inserting values into sheet as new column\n",
    "DRS_data=pd.merge(rs_sheet_info,DRS_file,on=['Site','Hole','depth-m'])\n",
    "DRS_data=pd.merge(DRS_data,rs_sheet_lith,left_on=['ID'],right_on=['name'],how='left')\n",
    "DRS_data.to_csv('DRS_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv('DRS_data.csv') # <--Replace the folder and file names with the merge results csv file \n",
    "\n",
    "coercivity=pd.read_csv('2023-09-29_irm_gradient.csv')\n",
    "coercivity.drop(coercivity[coercivity['max_d2']>2000].index, inplace=True)\n",
    "coercivity['depth-m']=coercivity['depth-m']-1+0.01\n",
    "coercivity['site']=coercivity.site+coercivity.hole\n",
    "coercivity=coercivity.drop('hole', axis=1)\n",
    "\n",
    "hysteresis=pd.read_csv('2023-09-29_hysteresis_parameters_by_lithology.csv')\n",
    "hysteresis['depth-m']=hysteresis['top depth csf-a (m)']+0.01\n",
    "hysteresis['site']=hysteresis.site+hysteresis.hole\n",
    "hysteresis=hysteresis.drop('hole', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=raw_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['site_hole']=raw_data.Site+raw_data.Hole\n",
    "print(raw_data.site_hole.unique())\n",
    "site_holes=raw_data.site_hole.unique()\n",
    "first_derivatives=dict(zip(raw_data.site_hole.unique(),np.zeros(raw_data.site_hole.unique().shape)))\n",
    "km_first_derivatives=dict(zip(raw_data.site_hole.unique(),np.zeros(raw_data.site_hole.unique().shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=11,window='hanning'):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "    \n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal \n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the begining and end part of the output signal.\n",
    "    \n",
    "    input:\n",
    "        x: the input signal \n",
    "        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "            flat window will produce a moving average smoothing.\n",
    "\n",
    "    output:\n",
    "        the smoothed signal\n",
    "        \n",
    "    example:\n",
    "\n",
    "    t=linspace(-2,2,0.1)\n",
    "    x=sin(t)+randn(len(t))*0.1\n",
    "    y=smooth(x)\n",
    "    \n",
    "    see also: \n",
    "    \n",
    "    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "    scipy.signal.lfilter\n",
    " \n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise (ValueError, \"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise (ValueError, \"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise (ValueError, \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('np.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n",
    "\n",
    "def first_derivative_smooth(reflectance,wavelength,wl=5,filter='flat'):\n",
    "    first=int((wl-1)/2)\n",
    "    smoothed_reflectance = smooth(reflectance,wl,filter)[first:-first]\n",
    "    return pd.Series(data=np.gradient(smoothed_reflectance,wavelength),index=wavelength)\n",
    "\n",
    "def first_derivative(reflectance,wavelength):\n",
    "    return pd.Series(data=np.gradient(reflectance,wavelength),index=wavelength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_param=99\n",
    "def kubelka_munk(x):\n",
    "    x = x/100\n",
    "    return ((1-x)**2)/(2*x)\n",
    "\n",
    "data_columns=['depth-m']+[str(x) for x in np.linspace(400,850,num=226,dtype='int')]\n",
    "\n",
    "for sh in site_holes:\n",
    "    print('Working on: ',sh)\n",
    "    sh_data=raw_data.loc[(raw_data.site_hole==sh),[(x in data_columns) for x in raw_data.columns]]\n",
    "    sh_kubelka_munk=sh_data\n",
    "    sh_kubelka_munk.loc[:,'400':'850']=sh_data.loc[:,'400':'850'].applymap(kubelka_munk)\n",
    "    wavelength=[float(i) for i in sh_data.drop('depth-m',axis=1).columns]\n",
    "    sh_grad_smooth=sh_data.drop('depth-m',axis=1).apply(first_derivative_smooth,axis=1,raw=True,wavelength=wavelength,wl=smooth_param)\n",
    "    sh_grad_smooth['depth-m']=sh_data['depth-m']\n",
    "    first_derivatives[sh]=sh_grad_smooth\n",
    "    \n",
    "    sh_grad_km_smooth=sh_kubelka_munk.drop('depth-m',axis=1).apply(first_derivative_smooth,axis=1,raw=True,wavelength=wavelength,wl=smooth_param)\n",
    "    sh_grad_km_smooth['depth-m']=sh_kubelka_munk['depth-m']\n",
    "    km_first_derivatives[sh]=sh_grad_km_smooth\n",
    "   \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_first_derivatives_df=pd.DataFrame.from_dict({(x,y):km_first_derivatives[x][y]\n",
    "                                                for x in km_first_derivatives.keys()\n",
    "                                                for y in km_first_derivatives[x].keys()},\n",
    "                                            orient='columns')\n",
    "km_first_derivatives_df=km_first_derivatives_df.stack(level=0)\n",
    "km_first_derivatives_df=km_first_derivatives_df.droplevel(0)\n",
    "km_first_derivatives_df=km_first_derivatives_df.drop_duplicates()\n",
    "\n",
    "site_info_DRS=km_first_derivatives_df['depth-m'].reset_index()\n",
    "#takes index and depth into new data frame and makes them columns\n",
    "site_info_DRS=site_info_DRS.rename(columns={'index':'site'})\n",
    "site_info_DRS['label']=site_info_DRS['site']+'-'+site_info_DRS['depth-m'].astype('str')\n",
    "#adds new column and assigns to data from site_info (astype- convert to string)\n",
    "site_info_DRS.reset_index(inplace=True)\n",
    "\n",
    "km_first_derivatives_df=km_first_derivatives_df.drop(('depth-m'),axis=1)\n",
    "min_value_DRS=km_first_derivatives_df.values.min(axis=None)\n",
    "km_first_derivatives_shifted=km_first_derivatives_df-min_value_DRS\n",
    "\n",
    "km_first_derivatives_plus_site_info=km_first_derivatives_df.reset_index()\n",
    "km_first_derivatives_plus_site_info=km_first_derivatives_plus_site_info.rename(columns={'index':'site'})\n",
    "km_first_derivatives_plus_site_info=pd.merge(site_info_DRS,km_first_derivatives_plus_site_info,on=['site'])\n",
    "\n",
    "km_derivatives_coercivity=pd.merge(km_first_derivatives_plus_site_info,coercivity, on=['site','depth-m'])\n",
    "site_info_DRS_coercivity=km_derivatives_coercivity[['site','depth-m','label']].reset_index()\n",
    "\n",
    "\n",
    "km_derivatives_coercivity_hysteresis=pd.merge(km_derivatives_coercivity,hysteresis, on=['site','depth-m'])\n",
    "site_info_DRS_coercivity_hysteresis=km_derivatives_coercivity_hysteresis[['site','depth-m','label']].reset_index()\n",
    "\n",
    "km_derivatives_coercivity=km_derivatives_coercivity.drop(['index','depth-m','site','label','max_d2','core','section','interval','Unnamed: 0','id'], axis=1)\n",
    "min_value_DRS_coercivity=km_derivatives_coercivity.values.min(axis=None)\n",
    "km_derivatives_coercivity_shifted=km_derivatives_coercivity-min_value_DRS_coercivity\n",
    "\n",
    "km_derivatives_coercivity_hysteresis=km_derivatives_coercivity_hysteresis.drop(['index','depth-m','site','label','max_d2',\n",
    "    'core_x','section_x','interval_x','Unnamed: 0_x','id_x','Unnamed: 0_y','id_y','core_y','section_y','interval_y','top [cm]','bottom [cm]','lithology principal name','top depth csf-a (m)'], axis=1)\n",
    "min_value_DRS_coercivity_hysteresis=km_derivatives_coercivity_hysteresis.values.min(axis=None)\n",
    "km_derivatives_coercivity_hysteresis_shifted=km_derivatives_coercivity_hysteresis-min_value_DRS_coercivity_hysteresis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
